{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.chdir(\"structural_optimization\")\n",
    "#os.chdir(\"smo\")\n",
    "#os.chdir(\"anja3\")\n",
    "#os.chdir(\"driver_glance_behaviour\")\n",
    "os.chdir(\"cov\")\n",
    "lvl = 0\n",
    "\n",
    "work_dict = pickle.load(open(\"work_dict_lvl{}.p\".format(lvl), \"rb\"))\n",
    "#work_dict = pickle.load(open(\"work_dict_test_lvl1.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10939"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#work_dict = pickle.load(open(\"work_dict_lvl1.p\", \"rb\"))\n",
    "#work_dict = pickle.load(open(\"work_dict_lvl2.p\", \"rb\"))\n",
    "#work_dict = pickle.load(open(\"crashworthiness/work_dict_lvl3.p\", \"rb\"))\n",
    "len(work_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_dois(work):\n",
    "    result = []\n",
    "    if \"reference\" in work:\n",
    "        for ref in work[\"reference\"]:\n",
    "            if \"DOI\" in ref:\n",
    "                result.append(ref[\"DOI\"])\n",
    "        #if \"DOI\" in work:\n",
    "            #result.append(work[\"DOI\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "issn_counts = defaultdict(int)\n",
    "issn2journalTitle = {}\n",
    "\n",
    "for work in work_dict.values():\n",
    "    if \"ISSN\" in work:\n",
    "        for issn in work[\"ISSN\"]:\n",
    "            issn_counts[issn] += 1\n",
    "            if not issn in issn2journalTitle:\n",
    "                if \"container-title\" in work:\n",
    "                    issn2journalTitle[issn] = \";\".join(work[\"container-title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pd.DataFrame(index=issn_counts.keys(), data=issn_counts)\n",
    "#pd.DataFrame(index=issn_counts.keys(), data={count: issn for issn, count in issn_counts.items()})\n",
    "df = pd.DataFrame(index=issn_counts.keys(), columns=[\"Titel\", \"Publikationen\"])\n",
    "for issn, count in sorted(issn_counts.items(), key=itemgetter(1), reverse=True):\n",
    "    df.at[issn, \"Titel\"] = issn2journalTitle[issn]\n",
    "    df.at[issn, \"Publikationen\"] = count\n",
    "\n",
    "df.to_excel(\"journal_list.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#jt = set()\n",
    "#f = open(\"journal_list.csv\", \"w\", encoding=\"utf-8\")\n",
    "#f.write(\"Count\\tJournalTitle\\tISSN\\n\")\n",
    "#for issn, count in sorted(issn_counts.items(), key=itemgetter(1), reverse=True):\n",
    "#    if not (count, issn2journalTitle[issn]) in jt:\n",
    "#        print(count, issn, issn2journalTitle[issn])\n",
    "#        jt.add((count, issn2journalTitle[issn]))\n",
    "#        f.write(\"{}\\t{}\\t{}\\n\".format(count, issn2journalTitle[issn], issn))\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5550"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references_list = []\n",
    "ref_counts = defaultdict(int)\n",
    "for work in work_dict.values():\n",
    "    #print(work)\n",
    "    work_refs = get_reference_dois(work)\n",
    "    #print(work_refs)\n",
    "    #break\n",
    "    for doi in work_refs:\n",
    "        ref_counts[doi] += 1\n",
    "    work_refs.append(work[\"DOI\"])\n",
    "    if len(work_refs) > 2:\n",
    "        references_list.append(work_refs)\n",
    "len(references_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_counts_by_year = defaultdict(list)\n",
    "for doi, count in ref_counts.items():\n",
    "    #work = work_dict[doi]\n",
    "    try:        \n",
    "        y = work_dict[doi][\"issued\"][\"date-parts\"][0][0]\n",
    "        if y:\n",
    "            ref_counts_by_year[y].append(count)\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fbockni\\AppData\\Local\\Continuum\\anaconda3\\envs\\new_pytorch_100\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\fbockni\\AppData\\Local\\Continuum\\anaconda3\\envs\\new_pytorch_100\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(2020, 3.6271186440677967, 236, 8.351749320351285, 0.2757009345794392),\n",
       "  (2019, 1.1666666666666667, 132, 2.6863492751597198, 0.8571428571428571)],\n",
       " nan)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_ref_count = defaultdict(int)\n",
    "median_ref_count = defaultdict(int)\n",
    "var_ref_count = defaultdict(int)\n",
    "n_ref_count = defaultdict(int)\n",
    "lambda_mle_ref_count = defaultdict(int)\n",
    "percentile_ref_count = defaultdict(int)\n",
    "for year, counts in ref_counts_by_year.items():\n",
    "    if len(counts) > 20:\n",
    "        avg_ref_count[year] = np.average(counts)\n",
    "        median_ref_count[year] = np.median(counts)\n",
    "        var_ref_count[year] = np.var(counts)\n",
    "        n_ref_count[year] = len(counts)\n",
    "        lambda_mle_ref_count[year] = len(counts) / sum(counts)\n",
    "        percentile_ref_count[year] = - np.log(0.1) / lambda_mle_ref_count[year]\n",
    "        \n",
    "#np.average(ref_counts_by_year[2017])\n",
    "\n",
    "sorted(avg_ref_count.items(), reverse=True)\n",
    "sorted(median_ref_count.items(), reverse=True)\n",
    "sorted(var_ref_count.items(), reverse=True)\n",
    "\n",
    "lambda_mle_ref_count_default = np.median([l for y, l in lambda_mle_ref_count.items() if y < 2000])\n",
    "\n",
    "x = sorted(zip(avg_ref_count.keys(), avg_ref_count.values(), median_ref_count.values(), var_ref_count.values(), n_ref_count.values()), reverse=True)\n",
    "#pprint(x)\n",
    "x = sorted(zip(avg_ref_count.keys(), avg_ref_count.values(), n_ref_count.values(), percentile_ref_count.values(), lambda_mle_ref_count.values()), reverse=True)\n",
    "#print(x)\n",
    "x, lambda_mle_ref_count_default\n",
    "\n",
    "\n",
    "#lambda_mle_ref_count_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130035, 378)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_count_normalized = defaultdict(float)\n",
    "\n",
    "for doi, count in ref_counts.items():\n",
    "    try:\n",
    "        year = work_dict[doi][\"issued\"][\"date-parts\"][0][0]\n",
    "        if year in lambda_mle_ref_count:\n",
    "            if year >= 2000:\n",
    "                l = lambda_mle_ref_count[year]\n",
    "            else:\n",
    "                l = lambda_mle_ref_count_default\n",
    "        else:\n",
    "            l = lambda_mle_ref_count_default\n",
    "        if l < lambda_mle_ref_count_default:\n",
    "            l = lambda_mle_ref_count_default\n",
    "        ref_count_normalized[doi] = l*count\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "len(ref_counts), len(ref_count_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 17:03:28,202 : INFO : collecting all words and their counts\n",
      "2020-03-17 17:03:28,202 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-17 17:03:28,251 : INFO : collected 134546 word types from a corpus of 145148 raw words and 5550 sentences\n",
      "2020-03-17 17:03:28,251 : INFO : Loading a fresh vocabulary\n",
      "2020-03-17 17:03:28,297 : INFO : effective_min_count=3 retains 1374 unique words (1% of original 134546, drops 133172)\n",
      "2020-03-17 17:03:28,297 : INFO : effective_min_count=3 leaves 7171 word corpus (4% of original 145148, drops 137977)\n",
      "2020-03-17 17:03:28,307 : INFO : deleting the raw counts dictionary of 134546 items\n",
      "2020-03-17 17:03:28,307 : INFO : sample=0.001 downsamples 32 most-common words\n",
      "2020-03-17 17:03:28,307 : INFO : downsampling leaves estimated 6600 word corpus (92.1% of prior 7171)\n",
      "2020-03-17 17:03:28,317 : INFO : estimated required memory for 1374 words and 300 dimensions: 3984600 bytes\n",
      "2020-03-17 17:03:28,317 : INFO : resetting layer weights\n",
      "2020-03-17 17:03:28,576 : INFO : training model with 16 workers on 1374 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=1000\n",
      "2020-03-17 17:03:28,601 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:28,607 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:28,612 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:28,617 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:28,622 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:28,622 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:28,636 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:28,636 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:28,636 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:28,636 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:28,641 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:28,641 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:28,641 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:28,641 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:28,641 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:28,706 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:28,711 : INFO : EPOCH - 1 : training on 145148 raw words (6575 effective words) took 0.1s, 57954 effective words/s\n",
      "2020-03-17 17:03:28,726 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:28,736 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:28,741 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:28,741 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:28,751 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:28,751 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:28,756 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:28,756 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:28,756 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:28,761 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:28,761 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:28,761 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:28,761 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:28,766 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:28,766 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:28,832 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:28,832 : INFO : EPOCH - 2 : training on 145148 raw words (6608 effective words) took 0.1s, 60866 effective words/s\n",
      "2020-03-17 17:03:28,846 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:28,851 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:28,856 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:28,861 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:28,866 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:28,871 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:28,871 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:28,876 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:28,876 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:28,876 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:28,876 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:28,876 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:28,881 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:28,881 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:28,881 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:28,977 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:28,977 : INFO : EPOCH - 3 : training on 145148 raw words (6622 effective words) took 0.1s, 48470 effective words/s\n",
      "2020-03-17 17:03:28,996 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:29,006 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:29,011 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:29,011 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:29,011 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:29,016 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:29,021 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:29,031 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:29,031 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:29,036 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:29,036 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:29,036 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:29,036 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:29,036 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:29,036 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:29,127 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:29,127 : INFO : EPOCH - 4 : training on 145148 raw words (6598 effective words) took 0.1s, 45088 effective words/s\n",
      "2020-03-17 17:03:29,146 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:29,146 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:29,161 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:29,161 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:29,161 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:29,166 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:29,171 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:29,171 : INFO : worker thread finished; awaiting finish of 8 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 17:03:29,176 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:29,176 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:29,181 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:29,181 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:29,181 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:29,181 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:29,181 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:29,241 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:29,241 : INFO : EPOCH - 5 : training on 145148 raw words (6603 effective words) took 0.1s, 64020 effective words/s\n",
      "2020-03-17 17:03:29,267 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:29,272 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:29,272 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:29,277 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:29,277 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:29,282 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:29,287 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:29,291 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:29,291 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:29,291 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:29,291 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:29,296 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:29,296 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:29,296 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:29,296 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:29,346 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:29,346 : INFO : EPOCH - 6 : training on 145148 raw words (6586 effective words) took 0.1s, 70085 effective words/s\n",
      "2020-03-17 17:03:29,391 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:29,391 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:29,391 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:29,396 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:29,396 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:29,396 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:29,396 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:29,401 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:29,401 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:29,401 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:29,401 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:29,401 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:29,401 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:29,401 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:29,401 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:29,451 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:29,451 : INFO : EPOCH - 7 : training on 145148 raw words (6596 effective words) took 0.1s, 99307 effective words/s\n",
      "2020-03-17 17:03:29,486 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:29,491 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:29,491 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:29,491 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:29,491 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:29,496 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:29,496 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:29,496 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:29,496 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:29,501 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:29,501 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:29,501 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:29,501 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:29,501 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:29,501 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:29,546 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:29,546 : INFO : EPOCH - 8 : training on 145148 raw words (6609 effective words) took 0.1s, 102234 effective words/s\n",
      "2020-03-17 17:03:29,567 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:29,572 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:29,577 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:29,577 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:29,587 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:29,592 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:29,592 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:29,592 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:29,592 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:29,596 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:29,596 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:29,596 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:29,601 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:29,601 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:29,601 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:29,646 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:29,646 : INFO : EPOCH - 9 : training on 145148 raw words (6606 effective words) took 0.1s, 75039 effective words/s\n",
      "2020-03-17 17:03:29,656 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:29,672 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:29,676 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:29,676 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:29,676 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:29,681 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:29,681 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:29,686 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:29,691 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:29,691 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:29,691 : INFO : worker thread finished; awaiting finish of 5 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 17:03:29,691 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:29,691 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:29,691 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:29,691 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:29,746 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:29,746 : INFO : EPOCH - 10 : training on 145148 raw words (6617 effective words) took 0.1s, 74893 effective words/s\n",
      "2020-03-17 17:03:29,756 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:29,766 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:29,766 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:29,771 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:29,776 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:29,776 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:29,786 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:29,791 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:29,791 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:29,791 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:29,791 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:29,791 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:29,791 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:29,796 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:29,796 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:29,841 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:29,847 : INFO : EPOCH - 11 : training on 145148 raw words (6594 effective words) took 0.1s, 74541 effective words/s\n",
      "2020-03-17 17:03:29,866 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:29,866 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:29,876 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:29,881 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:29,886 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:29,891 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:29,891 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:29,891 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:29,891 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:29,891 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:29,896 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:29,896 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:29,896 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:29,896 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:29,901 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:29,947 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:29,947 : INFO : EPOCH - 12 : training on 145148 raw words (6575 effective words) took 0.1s, 74327 effective words/s\n",
      "2020-03-17 17:03:29,956 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:29,972 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:29,972 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:29,976 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:29,976 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:29,986 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:29,986 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:29,991 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:29,991 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:29,991 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:29,996 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:29,996 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:29,996 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:29,996 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:29,996 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:30,041 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:30,041 : INFO : EPOCH - 13 : training on 145148 raw words (6586 effective words) took 0.1s, 77347 effective words/s\n",
      "2020-03-17 17:03:30,071 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:30,071 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:30,076 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:30,076 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:30,076 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:30,076 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:30,081 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:30,081 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:30,081 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:30,081 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:30,086 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:30,086 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:30,086 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:30,086 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:30,091 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:30,141 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:30,141 : INFO : EPOCH - 14 : training on 145148 raw words (6583 effective words) took 0.1s, 76430 effective words/s\n",
      "2020-03-17 17:03:30,157 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:30,172 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:30,177 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:30,177 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:30,177 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:30,181 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:30,186 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:30,186 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:30,186 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:30,191 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:30,191 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:30,191 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:30,191 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:30,191 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 17:03:30,191 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:30,241 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:30,241 : INFO : EPOCH - 15 : training on 145148 raw words (6586 effective words) took 0.1s, 72037 effective words/s\n",
      "2020-03-17 17:03:30,256 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:30,266 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:30,271 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:30,276 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:30,281 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:30,291 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:30,291 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:30,291 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:30,291 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:30,296 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:30,296 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:30,296 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:30,301 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:30,301 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:30,301 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:30,356 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:30,356 : INFO : EPOCH - 16 : training on 145148 raw words (6584 effective words) took 0.1s, 66342 effective words/s\n",
      "2020-03-17 17:03:30,376 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:30,386 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:30,386 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:30,397 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:30,397 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:30,401 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:30,406 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:30,436 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:30,436 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:30,441 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:30,441 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:30,446 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:30,446 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:30,446 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:30,446 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:30,507 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:30,507 : INFO : EPOCH - 17 : training on 145148 raw words (6612 effective words) took 0.1s, 51323 effective words/s\n",
      "2020-03-17 17:03:30,526 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:30,531 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:30,536 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:30,541 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:30,546 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:30,546 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:30,551 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:30,556 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:30,556 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:30,556 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:30,556 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:30,561 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:30,561 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:30,577 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:30,582 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:30,671 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:30,671 : INFO : EPOCH - 18 : training on 145148 raw words (6607 effective words) took 0.2s, 43275 effective words/s\n",
      "2020-03-17 17:03:30,701 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:30,711 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:30,711 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:30,716 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:30,721 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:30,721 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:30,726 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:30,731 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:30,731 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:30,731 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:30,731 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:30,736 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:30,736 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:30,736 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:30,736 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:30,820 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:30,820 : INFO : EPOCH - 19 : training on 145148 raw words (6608 effective words) took 0.1s, 50577 effective words/s\n",
      "2020-03-17 17:03:30,840 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-03-17 17:03:30,850 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-03-17 17:03:30,850 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-03-17 17:03:30,868 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-03-17 17:03:30,870 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-03-17 17:03:30,877 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-03-17 17:03:30,878 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-03-17 17:03:30,878 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-03-17 17:03:30,880 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-03-17 17:03:30,880 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-03-17 17:03:30,880 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-17 17:03:30,880 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-17 17:03:30,880 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-17 17:03:30,880 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-17 17:03:30,887 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-17 17:03:30,938 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-17 17:03:30,938 : INFO : EPOCH - 20 : training on 145148 raw words (6576 effective words) took 0.1s, 61353 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 17:03:30,946 : INFO : training on a 2902960 raw words (131931 effective words) took 2.4s, 55892 effective words/s\n",
      "2020-03-17 17:03:30,946 : INFO : saving Word2VecKeyedVectors object under vectors.rv, separately None\n",
      "2020-03-17 17:03:30,946 : INFO : not storing attribute vectors_norm\n",
      "2020-03-17 17:03:30,981 : INFO : saved vectors.rv\n"
     ]
    }
   ],
   "source": [
    "#model = Word2Vec(iter=1, size=300, window=1000, min_count=5, workers=4, sg=1)  # an empty model, no training yet\n",
    "#model.build_vocab(references_list)\n",
    "#model.train(references_list)\n",
    "\n",
    "load_model = False\n",
    "\n",
    "fname = \"vectors.rv\"\n",
    "#fname = \"testRVnew.rv\"\n",
    "\n",
    "\n",
    "#load_model = True\n",
    "\n",
    "#fname = \"vectors_db.rv\"\n",
    "\n",
    "if not load_model or not fname in os.listdir():\n",
    "    model = Word2Vec(references_list, size=300, window=1000, min_count=3, workers=16, sg=1, iter=20)\n",
    "    \n",
    "    reference_vectors = model.wv\n",
    "    reference_vectors.save(fname)\n",
    "    del model\n",
    "else:\n",
    "    reference_vectors = KeyedVectors.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reference_vectors.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference_vectors = model.wv\n",
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fname = \"vectors.rv\"\n",
    "#reference_vectors.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_dict: 1374\n"
     ]
    }
   ],
   "source": [
    "X_dict = {}\n",
    "\n",
    "for doi in reference_vectors.vocab.keys():\n",
    "    newX = {}\n",
    "    newX[\"lvl\"] = 0\n",
    "    newX[\"vec\"] = reference_vectors[doi]\n",
    "    X_dict[doi] = newX\n",
    "print(\"Size of X_dict:\", len(X_dict))\n",
    "if 0:\n",
    "    for lvl in range(1,2):\n",
    "        print (\"Level:\", lvl)\n",
    "        dois = set(work_dict.keys() - X_dict.keys())\n",
    "        print(\"No. DOIs:\", len(dois), \"/\", len(work_dict))\n",
    "        for doi in dois:\n",
    "            work = work_dict[doi]\n",
    "        #for doi, work in work_dict.items():\n",
    "            if not doi in X_dict:\n",
    "                ref_dois = get_reference_dois(work)\n",
    "                ref_dois = set(ref_dois) & X_dict.keys()            \n",
    "                if len(ref_dois) >= 3:\n",
    "                    vec = np.zeros(300)\n",
    "                    for ref_doi in ref_dois:\n",
    "                        #print(work_dict[ref_doi])\n",
    "                        vec = vec + X_dict[ref_doi][\"vec\"]\n",
    "                    vec = vec / len(ref_dois)\n",
    "                    newX = {}\n",
    "                    newX[\"lvl\"] = lvl\n",
    "                    newX[\"vec\"] = vec\n",
    "                    X_dict[doi] = newX\n",
    "                pass\n",
    "        print(\"Size of X_dict:\", len(X_dict))\n",
    "\n",
    "    \n",
    "    work_dict0 = pickle.load(open(\"work_dict_lvl0.p\", \"rb\"))\n",
    "    len(X_dict), len(work_dict0), len(work_dict), len(X_dict.keys() & work_dict.keys()), len(X_dict.keys() & work_dict0.keys()), len(reference_vectors.vocab.keys() & work_dict0.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reference_vectors.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 possible\n",
      "120 selected\n",
      "120 selected\n"
     ]
    }
   ],
   "source": [
    "dois_to_plot = list(reference_vectors.vocab.keys() & work_dict.keys())\n",
    "#dois_to_plot = list(reference_vectors.vocab.keys())\n",
    "print(len(dois_to_plot), \"possible\")\n",
    "\n",
    "def doi_plot_filter(doi):\n",
    "    if not \"title\" in work_dict[doi]:\n",
    "        return False\n",
    "    if len(work_dict[doi][\"title\"]) == 0:\n",
    "        return False\n",
    "    if not work_dict[doi][\"issued\"][\"date-parts\"][0][0]:\n",
    "        return False\n",
    "    if work_dict[doi][\"issued\"][\"date-parts\"][0][0] >= 2014:\n",
    "        return True\n",
    "    elif ref_counts[doi] >= 20:\n",
    "        return True\n",
    "    elif work_dict[doi][\"issued\"][\"date-parts\"][0][0] >= 2008 and ref_counts[doi] >= 10:\n",
    "        return True\n",
    "    elif work_dict[doi][\"issued\"][\"date-parts\"][0][0] >= 2012 and ref_counts[doi] >= 5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "test_dois = [\n",
    "    \"10.1065/lca2008.02.376\",\n",
    "    \"10.1016/j.cirp.2010.05.004\",\n",
    "    #\"10.1007/s11367-014-0788-0\",\n",
    "    \"10.1007/978-3-642-01421-5\",\n",
    "    \"10.1108/09564230310474138\",\n",
    "    \"10.1080/17517575.2012.683812\",\n",
    "    \"10.1002/bse.414\",\n",
    "    \"10.1111/j.1468-2370.2007.00202.x\"\n",
    "]\n",
    "\n",
    "test_dois = [\n",
    "    \"10.1371/journal.pone.0031092\",\n",
    "    \"10.1093/nar/gks017\",\n",
    "    \"10.1016/j.chembiol.2012.01.015\",\n",
    "    \"10.1111/j.1462-2920.2012.02719.x\",\n",
    "    \"10.1021/ja3072397\",\n",
    "    \"10.1371/journal.ppat.1002760\",\n",
    "    \"10.1111/1462-2920.12024\",\n",
    "    \"10.1371/journal.ppat.1004744\",\n",
    "    \"10.1186/1475-2859-13-88\",\n",
    "    \"10.1016/j.mimet.2011.11.015\",\n",
    "    \"10.1099/jmm.0.05069-0\"\n",
    "]\n",
    "\n",
    "test_dois = [\n",
    "    \"10.1007/s001580050176\", # A 99 line topology optimization code written in Matlab\n",
    "    \"10.1038/451652a\" # Building better batteries\n",
    "]\n",
    "\n",
    "test_dois = [\n",
    "    \"10.1038/nature14539\", # Deep learning\n",
    "    \"10.1038/nature16961\",\n",
    "    \"10.1038/nature21056\"\n",
    "]\n",
    "\n",
    "test_dois = [\n",
    "    \"10.1016/j.tcs.2014.11.028\",\n",
    "    \"10.1007/978-3-662-05094-1\",\n",
    "    \"10.1038/nature14422\",\n",
    "    \"10.1126/science.1165893\",\n",
    "    \"10.1109/4235.996017\",\n",
    "    \"10.1007/978-3-642-17339-4\"\n",
    "]\n",
    "\n",
    "\n",
    "test_dois = [\n",
    "    \"10.1016/j.addma.2016.06.010\", # Topology optimization of 3D self-supporting structures for additive manufacturing\n",
    "    \"10.1007/s00158-015-1261-9\" #Topology and shape optimization methods using evolutionary algorithms: a review\n",
    "]\n",
    "\n",
    "#test_dois = [\n",
    "#    \"10.1016/j.actamat.2016.07.019\" # Additive manufacturing of metals\n",
    "#]\n",
    "\n",
    "#test_dois = [\n",
    "#    \"10.1115/1.4004465\", # Multidisciplinary Design Optimization for Complex Engineered Systems: Report From a National Science Foundation Workshop\n",
    "#    \"10.1007/s00158-012-0763-y\", # Extensions to the design structure matrix for the description of multidisciplinary design, analysis, and optimization processes\n",
    "#    \"10.1016/j.paerosci.2011.05.001\"\n",
    "#    \n",
    "#]\n",
    "\n",
    "test_dois = [\n",
    "    \"10.1016/j.cad.2015.04.001\",\n",
    "]\n",
    "\n",
    "test_dois = [\n",
    "    \"10.1561/1500000011\",\n",
    "    \"10.1145/1014052.1014073\",\n",
    "]\n",
    "\n",
    "test_dois = [\n",
    "    \"10.1016/j.trf.2014.06.016\"   # Effects of adaptive cruise control and highly automated driving on workload and situation awareness: A review of the empirical evidence\n",
    "]\n",
    "\n",
    "test_dois = [\n",
    "    \"10.1126/sciadv.1602614\", #Data-driven discovery of partial differential equations\n",
    "    #\"10.1146/annurev-fluid-010816-060042\",\n",
    "    \"10.1063/1.4927765\",\n",
    "    \"10.1017/jfm.2016.615\"\n",
    "]\n",
    "\n",
    "for doi in test_dois:\n",
    "    if doi in work_dict:\n",
    "        print(work_dict[doi][\"title\"])\n",
    "\n",
    "def doi_plot_filter2(doi, min_sim=0.35):\n",
    "    \n",
    "    \n",
    "    #min_sim = 0.35\n",
    "    \n",
    "    for test_doi in test_dois:\n",
    "        if reference_vectors.similarity(doi, test_doi) > min_sim:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "#dois_to_plot = [doi for doi, count in sorted(ref_count_normalized.items(), key=itemgetter(1), reverse=True)][:3000]\n",
    "        \n",
    "#dois_to_plot = [doi for doi in dois_to_plot if doi_plot_filter(doi)]\n",
    "#dois_to_plot = [doi for doi in dois_to_plot if doi_plot_filter2(doi, 0.4)]\n",
    "\n",
    "dois_to_plot = [doi for doi in dois_to_plot if \"title\" in work_dict[doi]]\n",
    "dois_to_plot = [doi for doi in dois_to_plot if len(work_dict[doi][\"title\"]) > 0]\n",
    "dois_to_plot = [doi for doi in dois_to_plot if work_dict[doi][\"issued\"][\"date-parts\"][0][0]]\n",
    "dois_to_plot = [doi for doi in dois_to_plot if work_dict[doi][\"issued\"][\"date-parts\"][0][0] >= 2014]\n",
    "#dois_to_plot = [doi for doi in dois_to_plot if ref_counts[doi] >= 4]\n",
    "\n",
    "#filter_issn = \"2071-1050\" # Sustainability\n",
    "#filter_issn = \"1615-1488\" # SMO\n",
    "#filter_issn = \"0268-3768\"\n",
    "#dois_to_plot = [doi for doi in dois_to_plot if \"ISSN\" in work_dict[doi]]\n",
    "#dois_to_plot = [doi for doi in dois_to_plot if filter_issn in work_dict[doi][\"ISSN\"]]\n",
    "\n",
    "#test_dois = [\"10.1016/j.actamat.2016.02.014\"] # Laser powder-bed fusion additive manufacturing: Physics of complex melt flow and formation mechanisms of pores, spatter, and denudation zones\n",
    "\n",
    "#dois_to_plot = [doi for doi in dois_to_plot if doi_plot_filter2(doi, 0.8)]\n",
    "\n",
    "print(len(dois_to_plot), \"selected\")\n",
    "\n",
    "dois_to_plot = [doi for doi in dois_to_plot if doi in ref_count_normalized]\n",
    "#dois_to_plot = [doi for doi in dois_to_plot if ref_count_normalized[doi] >= 1.5]\n",
    "\n",
    "#dois_to_plot = [doi for doi, count in sorted([doi, ref_count_normalized[doi] for doi in dois_to_plot], key=itemgetter(1), reverse=True)][:3000]\n",
    "\n",
    "a = [(doi, ref_count_normalized[doi]) for doi in dois_to_plot]\n",
    "b = sorted(a, key=itemgetter(1), reverse=True)[:2500]\n",
    "dois_to_plot = [doi for doi, count in b]\n",
    "\n",
    "print(len(dois_to_plot), \"selected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reference_vectors[dois_to_plot]\n",
    "\n",
    "tsne = TSNE(n_components=2, metric=\"cosine\")\n",
    "X_embedded = tsne.fit_transform(X)\n",
    "#X_embedded = TSNE(n_components=2).fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    dois_to_plot = list(X_dict.keys() & work_dict0.keys())\n",
    "    print(len(dois_to_plot))\n",
    "    X = np.zeros((len(dois_to_plot), 300))\n",
    "    for i, doi in enumerate(dois_to_plot):\n",
    "        X[i, :] = X_dict[doi][\"vec\"]\n",
    "\n",
    "    X_embedded = TSNE(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_embedded = TSNE(n_components=3, metric=\"cosine\").fit_transform(X)\n",
    "#C_embedded = TSNE(n_components=3).fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_embedded *= 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-197.39978, 196.79071)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#colors_r = np.floor(MinMaxScaler().fit_transform(C_embedded[:,0])*256)\n",
    "\n",
    "cmin = np.min(C_embedded)\n",
    "cmax = np.max(C_embedded)\n",
    "cmin, cmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[145., 153., 212.],\n",
       "       [131., 145., 256.],\n",
       "       [242., 192., 131.],\n",
       "       [ 92.,  16., 159.],\n",
       "       [231., 102., 183.],\n",
       "       [204.,  96., 213.],\n",
       "       [187., 175.,  37.],\n",
       "       [164., 190., 160.],\n",
       "       [226., 155., 105.],\n",
       "       [ 59.,  29., 159.],\n",
       "       [163.,  32.,  65.],\n",
       "       [123., 184., 192.],\n",
       "       [ 90.,  95., 122.],\n",
       "       [104.,  94.,  17.],\n",
       "       [167.,  18., 182.],\n",
       "       [214., 218., 147.],\n",
       "       [ 86., 146., 113.],\n",
       "       [121., 149.,  83.],\n",
       "       [201.,  70., 146.],\n",
       "       [160., 173., 119.],\n",
       "       [ 77., 154.,  13.],\n",
       "       [ 37.,  80., 170.],\n",
       "       [248., 122., 101.],\n",
       "       [180.,  70.,  87.],\n",
       "       [132.,  90., 174.],\n",
       "       [103., 240., 168.],\n",
       "       [182., 179., 208.],\n",
       "       [ 67.,  81., 189.],\n",
       "       [119., 134., 162.],\n",
       "       [ 86., 126.,  83.],\n",
       "       [136., 208., 224.],\n",
       "       [167., 126., 158.],\n",
       "       [190., 228.,  79.],\n",
       "       [181.,  29., 144.],\n",
       "       [170.,  16., 107.],\n",
       "       [111., 120., 200.],\n",
       "       [132.,  19., 131.],\n",
       "       [ 42., 105.,  55.],\n",
       "       [ 90., 164., 148.],\n",
       "       [152., 106., 207.],\n",
       "       [141., 238., 110.],\n",
       "       [227., 144.,  58.],\n",
       "       [133.,  80.,  63.],\n",
       "       [113., 190., 146.],\n",
       "       [ 56., 136.,  90.],\n",
       "       [113., 118., 130.],\n",
       "       [ 76., 177., 103.],\n",
       "       [ 58., 103.,  88.],\n",
       "       [125., 192.,  99.],\n",
       "       [ 88., 152., 222.],\n",
       "       [209., 191., 171.],\n",
       "       [137., 179.,  45.],\n",
       "       [231., 159., 143.],\n",
       "       [134., 208.,  56.],\n",
       "       [157.,  97.,  99.],\n",
       "       [151., 218., 143.],\n",
       "       [ 65., 195.,  63.],\n",
       "       [105.,  43.,  45.],\n",
       "       [ 99., 175.,  74.],\n",
       "       [ 17., 172., 143.],\n",
       "       [187., 137., 239.],\n",
       "       [193., 187.,  86.],\n",
       "       [116.,  65., 126.],\n",
       "       [ 19., 134., 184.],\n",
       "       [ 41., 213., 168.],\n",
       "       [ 56., 147., 155.],\n",
       "       [197.,  62., 186.],\n",
       "       [ 94., 132.,  48.],\n",
       "       [216.,  75., 103.],\n",
       "       [ 88.,  17., 120.],\n",
       "       [181., 135., 190.],\n",
       "       [ 81., 123., 155.],\n",
       "       [218., 142., 222.],\n",
       "       [ 86.,  71., 159.],\n",
       "       [199., 105.,  33.],\n",
       "       [ 27., 141.,  63.],\n",
       "       [152., 130.,  82.],\n",
       "       [133.,  43., 163.],\n",
       "       [163., 205., 103.],\n",
       "       [236., 152., 181.],\n",
       "       [145., 135., 120.],\n",
       "       [ 76.,  53., 224.],\n",
       "       [ 49., 114.,  11.],\n",
       "       [ 69.,  84.,  20.],\n",
       "       [ 81., 115.,   0.],\n",
       "       [127., 158., 127.],\n",
       "       [ 35., 163.,  95.],\n",
       "       [240., 195.,  78.],\n",
       "       [226., 119., 137.],\n",
       "       [148.,  66., 202.],\n",
       "       [ 75., 122., 193.],\n",
       "       [139.,  98., 132.],\n",
       "       [120., 101.,  96.],\n",
       "       [122.,  46.,  88.],\n",
       "       [179., 236., 192.],\n",
       "       [ 63.,  62., 125.],\n",
       "       [188., 123., 113.],\n",
       "       [183., 109.,  76.],\n",
       "       [ 54., 109., 133.],\n",
       "       [ 60., 138.,  53.],\n",
       "       [188., 227., 127.],\n",
       "       [175.,  97., 132.],\n",
       "       [158.,  64., 112.],\n",
       "       [178., 158., 141.],\n",
       "       [ 63., 179., 190.],\n",
       "       [106.,  42., 197.],\n",
       "       [146., 160., 169.],\n",
       "       [ 80.,  99.,  61.],\n",
       "       [188., 154.,  72.],\n",
       "       [ 73.,  57.,  82.],\n",
       "       [ 94., 203., 216.],\n",
       "       [ 72., 209., 146.],\n",
       "       [ 89., 217., 101.],\n",
       "       [ 24., 118.,  93.],\n",
       "       [147., 118.,  45.],\n",
       "       [158.,  69.,  25.],\n",
       "       [164.,  68., 157.],\n",
       "       [ 95., 156., 180.],\n",
       "       [104.,  76., 227.],\n",
       "       [189., 188., 128.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = (C_embedded - cmin) / (cmax - cmin) * 256\n",
    "colors = np.floor(colors)\n",
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.3305004, 5.8291836], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embedded[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"doi\", \"title\", \"year\", \"link\", \"authors\", \"containerTitles\", \"refCount\", \"xval\", \"yval\", \"rgb\", \"clusterNo\"]\n",
    "df = pd.DataFrame(index=dois_to_plot, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "#\n",
    "#max_d = 0.2\n",
    "#\n",
    "#Z = linkage(X, method='average', metric='cosine')\n",
    "##Z = linkage(X, method='complete', metric='cosine')\n",
    "#clLabels = fcluster(Z, max_d, criterion='distance') - 1\n",
    "##clLabels = fcluster(Z, 50, criterion='maxclust') - 1\n",
    "#\n",
    "#cluster_ids = sorted(np.unique(clLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.cluster import DBSCAN\n",
    "#\n",
    "#clLabels = DBSCAN(eps=0.1, min_samples=5, metric=\"cosine\").fit_predict(X)\n",
    "#cluster_ids = sorted(np.unique(clLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.sqrt(len(dois_to_plot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Initialization 0\n",
      "Initialization converged: True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "n = int(np.sqrt(len(dois_to_plot)))\n",
    "print(n)\n",
    "\n",
    "gmm = GaussianMixture(n_components=n, verbose=True)\n",
    "\n",
    "clLabels = gmm.fit_predict(X)\n",
    "cluster_ids = sorted(np.unique(clLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 12),\n",
       " (1, 13),\n",
       " (2, 11),\n",
       " (3, 5),\n",
       " (4, 34),\n",
       " (5, 18),\n",
       " (6, 8),\n",
       " (7, 7),\n",
       " (8, 10),\n",
       " (9, 2)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(cl, np.sum(clLabels==cl)) for cl in cluster_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 6, 0, 8, 3, 3, 3, 5, 3, 1, 4, 1, 5, 7, 1, 6, 0, 5, 4, 5, 9, 7,\n",
       "        6, 4, 1, 4, 8, 7, 8, 2, 4, 5, 4, 1, 5, 4, 1, 2, 4, 4, 0, 5, 8, 4,\n",
       "        2, 4, 5, 2, 4, 4, 6, 4, 4, 4, 4, 0, 4, 0, 4, 3, 0, 0, 5, 1, 1, 5,\n",
       "        1, 2, 8, 8, 1, 5, 0, 9, 1, 2, 5, 4, 0, 6, 5, 8, 7, 7, 7, 5, 2, 8,\n",
       "        6, 4, 5, 4, 4, 0, 2, 4, 4, 4, 4, 2, 0, 4, 4, 5, 4, 8, 5, 2, 6, 7,\n",
       "        4, 1, 5, 2, 4, 6, 4, 4, 8, 0], dtype=int64), 9)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clLabels, np.max(cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import *\n",
    "from pprint import pprint\n",
    "import operator\n",
    "\n",
    "def phrase_lines(texts_temp):\n",
    "\n",
    "    phrases = Phrases(texts_temp, min_count=10, threshold=10)\n",
    "    bigram = Phraser(phrases)\n",
    "    texts_phrased = [bigram[t] for t in texts_temp]\n",
    "\n",
    "    phrase_scores = {}\n",
    "    for phrase, score in phrases.export_phrases(texts_temp):\n",
    "        phrase_scores[phrase] = score\n",
    "    print(\"Bigramme\")\n",
    "    pprint(sorted(phrase_scores.items(), key=operator.itemgetter(1), reverse=True)[:50])\n",
    "\n",
    "    texts_temp = texts_phrased\n",
    "    phrases = Phrases(texts_temp, min_count=5, threshold=7)\n",
    "    trigram = Phraser(phrases)\n",
    "    texts_phrased = [trigram[t] for t in texts_temp]\n",
    "\n",
    "    phrase_scores = {}\n",
    "    for phrase, score in phrases.export_phrases(texts_temp):\n",
    "        phrase_scores[phrase] = score\n",
    "    print(\"Trigramme\")\n",
    "    pprint(sorted(phrase_scores.items(), key=operator.itemgetter(1), reverse=True)[:50])\n",
    "\n",
    "    return texts_phrased, bigram, trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 17:03:33,221 : INFO : collecting all words and their counts\n",
      "2020-03-17 17:03:33,221 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2020-03-17 17:03:33,227 : INFO : collected 1310 word types from a corpus of 1085 words (unigram + bigrams) and 120 sentences\n",
      "2020-03-17 17:03:33,227 : INFO : using 1310 counts as vocab in Phrases<0 vocab, min_count=10, threshold=10, max_vocab_size=40000000>\n",
      "2020-03-17 17:03:33,227 : INFO : source_vocab length 1310\n",
      "2020-03-17 17:03:33,237 : INFO : Phraser built with 2 phrasegrams\n",
      "2020-03-17 17:03:33,267 : INFO : collecting all words and their counts\n",
      "2020-03-17 17:03:33,267 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2020-03-17 17:03:33,271 : INFO : collected 1317 word types from a corpus of 1035 words (unigram + bigrams) and 120 sentences\n",
      "2020-03-17 17:03:33,271 : INFO : using 1317 counts as vocab in Phrases<0 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "2020-03-17 17:03:33,271 : INFO : source_vocab length 1317\n",
      "2020-03-17 17:03:33,281 : INFO : Phraser built with 7 phrasegrams\n",
      "2020-03-17 17:03:33,287 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-03-17 17:03:33,291 : INFO : built Dictionary(525 unique tokens: ['coronaviruses:', 'emerging', 'genome', 'pathogenesis', 'replication,']...) from 120 documents (total 996 corpus positions)\n",
      "2020-03-17 17:03:33,291 : INFO : collecting document frequencies\n",
      "2020-03-17 17:03:33,291 : INFO : PROGRESS: processing document #0\n",
      "2020-03-17 17:03:33,291 : INFO : calculating IDF weights for 120 documents and 524 features (989 matrix non-zeros)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigramme\n",
      "[(b'novel coronavirus', 12.54399585921325), (b'2019 novel', 11.696428571428571)]\n",
      "Trigramme\n",
      "[(b'2019_novel coronavirus', 20.353636363636365),\n",
      " (b'novel_coronavirus (2019-ncov)', 13.17),\n",
      " (b'wuhan, china', 11.224431818181817)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "titles = []\n",
    "for doi in dois_to_plot:\n",
    "    pub = work_dict[doi]\n",
    "    if \"title\" in pub:\n",
    "        if len(pub[\"title\"]) > 0:\n",
    "            pub_title = [s for s in pub[\"title\"][0].lower().split() if not s in stopwords]\n",
    "            titles.append(pub_title)\n",
    "\n",
    "phrased_lines, bigram, trigram = phrase_lines(titles)\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for line in phrased_lines:\n",
    "    for term in line:\n",
    "        frequency[term] += 1\n",
    "\n",
    "dictionary = corpora.Dictionary(phrased_lines)\n",
    "\n",
    "corpus = [dictionary.doc2bow(line) for line in phrased_lines]\n",
    "\n",
    "tfidf = models.TfidfModel(corpus, normalize=True) # step 1 -- initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster No. 0\n",
      "Size: 12\n",
      "\n",
      "Cluster No. 1\n",
      "Size: 13\n",
      "\n",
      "Cluster No. 2\n",
      "Size: 11\n",
      "\n",
      "Cluster No. 3\n",
      "Size: 5\n",
      "\n",
      "Cluster No. 4\n",
      "Size: 34\n",
      "\n",
      "Cluster No. 5\n",
      "Size: 18\n",
      "\n",
      "Cluster No. 6\n",
      "Size: 8\n",
      "\n",
      "Cluster No. 7\n",
      "Size: 7\n",
      "\n",
      "Cluster No. 8\n",
      "Size: 10\n",
      "\n",
      "Cluster No. 9\n",
      "Size: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clSizes = []\n",
    "clTitles = defaultdict(list)\n",
    "\n",
    "for cl in cluster_ids:    \n",
    "    print(\"Cluster No.\", cl)\n",
    "    cl_dois = [doi for i, doi in enumerate(dois_to_plot) if clLabels[i] == cl]\n",
    "    print(\"Size:\", len(cl_dois))\n",
    "    for doi in cl_dois:\n",
    "        #pprint(work_dict[doi][\"title\"])\n",
    "        pub = work_dict[doi]\n",
    "        if \"title\" in pub:\n",
    "            if len(pub[\"title\"]) > 0:\n",
    "                pub_title = [s for s in pub[\"title\"][0].lower().split() if not s in stopwords]\n",
    "                clTitles[cl].append(pub_title)        \n",
    "    print()\n",
    "    clSizes.append(len(cl_dois))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster No. 0\n",
      "Size: 12\n",
      "Terms: \n",
      "fusion (2, 2, 0.24), genome (3, 2, 0.18), associated (4, 2, 0.17), respiratory (4, 2, 0.17), new (5, 2, 0.16), origin (5, 2, 0.16), emerging (7, 2, 0.14), coronaviruses (8, 2, 0.14), sars (8, 2, 0.14), coronavirus (22, 3, 0.13), compared (1, 1, 0.12), higher (1, 1, 0.12), reproductive (1, 1, 0.12), learning (1, 1, 0.12), past: (1, 1, 0.12), possible (1, 1, 0.12), severe (1, 1, 0.12), urgent (1, 1, 0.12), assessment (1, 1, 0.12), b (1, 1, 0.12)\n",
      "top_authors: [('Liu', 8), ('Wang', 7), ('Chen', 6), ('Zhang', 5), ('Wu', 4), ('Xu', 3), ('Li', 3), ('Zhu', 3), ('Jiang', 3), ('Shi', 3)]\n",
      "\n",
      "Cluster No. 1\n",
      "Size: 13\n",
      "Terms: \n",
      "2019ncov (5, 3, 0.23), coronavirus (22, 5, 0.21), transmission (7, 3, 0.21), (2019ncov) (2, 2, 0.20), global (4, 2, 0.17), evolution (4, 2, 0.17), health (6, 2, 0.16), outbreak (16, 3, 0.15), novel (6, 2, 0.15), emerging (7, 2, 0.14), wuhan,_china (8, 2, 0.13), coronaviruses: (1, 1, 0.12), pathogenesis (1, 1, 0.12), replication, (1, 1, 0.12), structure, (1, 1, 0.12), continuing (1, 1, 0.12), epidemic (1, 1, 0.12), latest (1, 1, 0.12), threat (1, 1, 0.12), 2019-new (1, 1, 0.12)\n",
      "top_authors: [('Li', 4), ('Wang', 4), ('Ciccozzi', 3), ('Chen', 2), ('Benvenuto', 2), ('Giovanetti', 2), ('Angeletti', 2), ('Zai', 2)]\n",
      "\n",
      "Cluster No. 2\n",
      "Size: 11\n",
      "Terms: \n",
      "2020 (5, 3, 0.20), 2019_novel_coronavirus (22, 5, 0.18), cluster (3, 2, 0.17), 19:00 (2, 2, 0.17), aedt (2, 2, 0.17), australia: (2, 2, 0.17), covid-19, (2, 2, 0.17), ending (2, 2, 0.17), february (2, 2, 0.17), reporting (2, 2, 0.17), week (2, 2, 0.17), epidemiology (3, 2, 0.16), report (3, 2, 0.16), study (4, 2, 0.14), health (6, 2, 0.14), case (5, 2, 0.14), wuhan, (8, 2, 0.12), cases (8, 2, 0.12), 99 (1, 1, 0.10), descriptive (1, 1, 0.10)\n",
      "top_authors: [('Chan', 3), ('Chen', 2), ('Liu', 2), ('Zhang', 2), ('Yang', 2), ('Poon', 2)]\n",
      "\n",
      "Cluster No. 3\n",
      "Size: 5\n",
      "Terms: \n",
      "bat (5, 4, 0.43), coronaviruses (8, 3, 0.27), sars-like (3, 2, 0.25), origin (5, 2, 0.21), coronavirus (22, 3, 0.18), circulating (1, 1, 0.16), shows (1, 1, 0.16), bats (1, 1, 0.16), characterization (1, 1, 0.16), infectivity (1, 1, 0.16), discovery (1, 1, 0.16), gene (1, 1, 0.16), pool (1, 1, 0.16), provides (1, 1, 0.16), rich (1, 1, 0.16), sars-related (1, 1, 0.16), diarrhoea (1, 1, 0.16), fatal (1, 1, 0.16), hku2-related (1, 1, 0.16), swine (1, 1, 0.16)\n",
      "top_authors: [('Wang', 7), ('Zhang', 6), ('Li', 6), ('Shi', 5), ('Zhu', 4), ('Yang', 3), ('Zhou', 3), ('Chen', 3), ('Ge', 2), ('Hu', 2)]\n",
      "\n",
      "Cluster No. 4\n",
      "Size: 34\n",
      "Terms: \n",
      "infection (8, 6, 0.20), china (14, 7, 0.18), coronavirus: (6, 5, 0.18), response (4, 4, 0.18), patients (9, 5, 0.16), pneumonia (18, 7, 0.16), ct (5, 4, 0.15), novel_coronavirus (16, 6, 0.15), clinical (11, 5, 0.14), 2019-ncov (13, 5, 0.13), prevention (4, 3, 0.13), chest (3, 3, 0.13), rt-pcr (3, 3, 0.13), outbreak (16, 5, 0.12), 2019 (5, 3, 0.12), covid-19 (5, 3, 0.12), virus (6, 3, 0.11), coronavirus (22, 5, 0.11), 2019_novel_coronavirus (22, 5, 0.10), changes (2, 2, 0.10)\n",
      "top_authors: [('Wang', 20), ('Liu', 14), ('Chen', 11), ('Yang', 10), ('Li', 10), ('Zhang', 8), ('Wu', 7), ('Xie', 6), ('Zhou', 6), ('Lin', 5)]\n",
      "\n",
      "Cluster No. 5\n",
      "Size: 18\n",
      "Terms: \n",
      "clinical (11, 5, 0.22), pneumonia (18, 6, 0.20), (2019-ncov) (7, 4, 0.20), infected (3, 3, 0.20), patients (9, 4, 0.20), 2019_novel_coronavirus (22, 6, 0.18), china: (7, 3, 0.15), characteristics (8, 3, 0.15), therapeutic (4, 2, 0.12), treatment (7, 2, 0.10), coronaviruses (8, 2, 0.10), wuhan,_china (8, 2, 0.10), wuhan, (8, 2, 0.10), novel_coronavirus_(2019-ncov) (9, 2, 0.09), causes (1, 1, 0.09), features (1, 1, 0.09), group (1, 1, 0.09), retrospective (1, 1, 0.09), series (1, 1, 0.09), investigation (1, 1, 0.09)\n",
      "top_authors: [('Zhang', 13), ('Wang', 12), ('Li', 9), ('Xu', 6), ('Hu', 6), ('Huang', 4), ('Yang', 4), ('Wu', 4), ('Yu', 4), ('Liu', 4)]\n",
      "\n",
      "Cluster No. 6\n",
      "Size: 8\n",
      "Terms: \n",
      "spike (6, 4, 0.31), cryo-em (2, 2, 0.21), strategies (2, 2, 0.21), binding (3, 2, 0.19), receptor (4, 2, 0.18), therapeutic (4, 2, 0.18), sars (8, 2, 0.14), coronavirus (22, 3, 0.14), domains (1, 1, 0.12), dynamic (1, 1, 0.12), glycoproteins (1, 1, 0.12), mers-cov (1, 1, 0.12), reveal (1, 1, 0.12), sars-cov (1, 1, 0.12), structures (1, 1, 0.12), conformation (1, 1, 0.12), prefusion (1, 1, 0.12), structure (1, 1, 0.12), calling (1, 1, 0.12), developing (1, 1, 0.12)\n",
      "top_authors: [('Zhang', 3), ('Wang', 3), ('Lu', 3), ('Shi', 3), ('Jiang', 3), ('Wu', 2), ('Graham', 2), ('Du', 2), ('Su', 2), ('Li', 2)]\n",
      "\n",
      "Cluster No. 7\n",
      "Size: 7\n",
      "Terms: \n",
      "c (1, 1, 0.16), functions (1, 1, 0.16), patterns (1, 1, 0.16), returning (1, 1, 0.16), 2019: (1, 1, 0.16), automatic (1, 1, 0.16), cohesion (1, 1, 0.16), emotion, (1, 1, 0.16), emotiw (1, 1, 0.16), engagement (1, 1, 0.16), prediction (1, 1, 0.16), tasks (1, 1, 0.16), euthymia (1, 1, 0.16), pursuit (1, 1, 0.16), drug-target-disease (1, 1, 0.16), factorization (1, 1, 0.16), interactions (1, 1, 0.16), relational (1, 1, 0.16), sources (1, 1, 0.16), tensor (1, 1, 0.16)\n",
      "top_authors: []\n",
      "\n",
      "Cluster No. 8\n",
      "Size: 10\n",
      "Terms: \n",
      "estimation (4, 4, 0.31), risk (5, 3, 0.21), analysis (6, 3, 0.20), 2020: (2, 2, 0.18), data-driven (2, 2, 0.18), infection: (2, 2, 0.18), novel_coronavirus_(2019-ncov) (9, 3, 0.18), early (3, 2, 0.17), number (3, 2, 0.17), data (3, 2, 0.17), china, (4, 2, 0.15), novel_coronavirus (16, 3, 0.14), transmission (7, 2, 0.13), wuhan, (8, 2, 0.12), cases (8, 2, 0.12), basic (1, 1, 0.11), phase (1, 1, 0.11), preliminary (1, 1, 0.11), reproduction (1, 1, 0.11), available (1, 1, 0.11)\n",
      "top_authors: [('Yang', 8), ('Wang', 7), ('Linton', 4), ('Kobayashi', 4), ('Hayashi', 4), ('Akhmetzhanov', 4), ('Jung', 4), ('Yuan', 4), ('Kinoshita', 4), ('Nishiura', 4)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clDict = {}\n",
    "\n",
    "for cl in cluster_ids[:]:\n",
    "    #cl_titles_phrased = []\n",
    "    clTokens = []\n",
    "    if len(clTitles[cl]) < 3:\n",
    "        continue\n",
    "    for title in clTitles[cl]:\n",
    "        title_phrased = trigram[bigram[title]]\n",
    "        #pprint(title_phrased)\n",
    "        #cl_titles_phrased.append(title_phrased)\n",
    "        for t in title_phrased:\n",
    "            clTokens.append(t)\n",
    "    #print(clTokens)\n",
    "    \n",
    "    cl_dois = [doi for i, doi in enumerate(dois_to_plot) if clLabels[i] == cl]\n",
    "    \n",
    "    clBow = dictionary.doc2bow(clTokens)\n",
    "    clFrequency = {}\n",
    "    for i, freq in clBow:\n",
    "        clFrequency[i] = freq\n",
    "    #print(clBow)\n",
    "    vec = sorted(tfidf[clBow], key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    print(\"Cluster No.\", cl)\n",
    "    print(\"Size:\", len(cl_dois))\n",
    "    print(\"Terms:\", \", \".join([\"{}\".format(dictionary[i]) for i, v in vec[:20] if v >= 100 and clFrequency[i] >= 5]))\n",
    "    print(\", \".join([\"{} ({}, {}, {:.2f})\".format(dictionary[i], frequency[dictionary[i]], clFrequency[i], v) for i, v in vec[:20]]))\n",
    "    #print([(dictionary[i], frequency[dictionary[i]], clFrequency[i], v) for i, v in vec])\n",
    "    \n",
    "    author_counts = defaultdict(int)\n",
    "    for doi in cl_dois:\n",
    "        work = work_dict[doi]\n",
    "        if \"author\" in work:\n",
    "            for author in work[\"author\"]:\n",
    "                if \"family\" in author:\n",
    "                    author_counts[author[\"family\"]] += 1\n",
    "    top_authors = sorted(author_counts.items(), key=itemgetter(1), reverse = True)\n",
    "    top_authors = [(a, n) for a,n in top_authors if n >= 2][:10]\n",
    "    print(\"top_authors:\", top_authors)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    \n",
    "    clDict[cl] = {}\n",
    "    clDict[cl][\"id\"] = cl\n",
    "    clDict[cl][\"size\"] = len(cl_dois)\n",
    "    clDict[cl][\"dois\"] = cl_dois\n",
    "    clDict[cl][\"vec\"] =  [(dictionary[i], frequency[dictionary[i]], clFrequency[i], v) for i, v in vec]\n",
    "    clDict[cl][\"top_authors\"] = top_authors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.95004162504212,\n",
       " 0.802624679775096,\n",
       " 0.6200510377447751,\n",
       " 0.28370212980097553,\n",
       " 0.1]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.colors import RGB\n",
    "\n",
    "#year2alpha = lambda year: 1.5 / (1. + np.exp(0.2*(2018 - year))) + 0.25\n",
    "year2alpha = lambda year: np.clip(2. / (1. + np.exp(0.10*(2018 - year))), a_min=0.1, a_max=None)\n",
    "\n",
    "\n",
    "for i, doi in enumerate(dois_to_plot):\n",
    "    pub = work_dict[doi]\n",
    "    df.at[doi, \"doi\"] = doi\n",
    "    df.at[doi, \"title\"] = pub[\"title\"][0]\n",
    "    if \"container-title\" in pub:\n",
    "        df.at[doi, \"containerTitle\"] = \";\".join(pub[\"container-title\"])\n",
    "    #line = pub[\"title\"][0]\n",
    "    #n = 8\n",
    "    #df.at[doi, \"title\"] = \"\\n\".join([line[j:j+n] for j in range(0, len(line), n)])\n",
    "    df.at[doi, \"issued\"] = \"-\".join([str(x) for x in pub[\"issued\"][\"date-parts\"][0]])\n",
    "    df.at[doi, \"refCount\"] = ref_counts[doi]\n",
    "    df.at[doi, \"refCountNormalized\"] = ref_count_normalized[doi]\n",
    "    #df.at[doi, \"scatterSize\"] = 6.+ np.power(ref_counts[doi], 0.8)\n",
    "    df.at[doi, \"scatterSize\"] = 4+2.5*np.power(ref_count_normalized[doi], 1.)\n",
    "    df.at[doi, \"xval\"] = X_embedded[i,0]\n",
    "    df.at[doi, \"yval\"] = X_embedded[i,1]\n",
    "    df.at[doi, \"rgb\"] = RGB(colors[i,0], colors[i,1], colors[i,2])\n",
    "    df.at[doi, \"alpha\"] = year2alpha(pub[\"issued\"][\"date-parts\"][0][0])\n",
    "    try:\n",
    "        pub_authors = [a[\"family\"] for a in pub[\"author\"]]\n",
    "        df.at[doi, \"author\"] = \", \".join(pub_authors)\n",
    "    except KeyError:\n",
    "        df.at[doi, \"author\"] = \"unavailable\"\n",
    "    \n",
    "    df.at[doi, \"clusterNo\"] = clLabels[i]\n",
    "    \n",
    "\n",
    "[year2alpha(y) for y in (2018, 2017, 2014, 2010, 2000, 1980)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"scatterSize\"] *= 0.75\n",
    "df[\"scatterSize\"] *= 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"xval\"].max()\n",
    "df[\"xval\"].min()\n",
    "x_span = df[\"xval\"].max() - df[\"xval\"].min()\n",
    "y_span = df[\"yval\"].max() - df[\"yval\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsne.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "genome, associated, respiratory, new, origin\n",
      "-2.745163083076477 5.540922403335571 12\n",
      "0.18661165856519799 0.16497473041201782 0.24907945075210838\n",
      "nan\n",
      "std too big\n",
      "\n",
      "1\n",
      "2019ncov, coronavirus, transmission, global, evolution\n",
      "3.382181167602539 5.644922733306885 13\n",
      "0.10140738342497796 0.0648985402595606 0.12039633690823946\n",
      "45.20961281708946\n",
      "std too big\n",
      "\n",
      "2\n",
      "2020, 2019_novel_coronavirus, cluster, epidemiology, report\n",
      "-4.566805362701416 -5.64125394821167 11\n",
      "0.043308210425842764 0.029827800981839937 0.05258610844796682\n",
      "15.714953271028035\n",
      "\n",
      "3\n",
      "bat, coronaviruses, sars-like, origin, coronavirus\n",
      "-0.5437428951263428 10.287522315979004 5\n",
      "0.03726471277301414 0.029495540896931963 0.04752521173922142\n",
      "nan\n",
      "\n",
      "4\n",
      "infection, china, coronavirus:, response, patients\n",
      "0.2362801656126976 0.2737620174884796 34\n",
      "0.1772500546280697 0.1359565069513308 0.22338700420584218\n",
      "35.56542056074766\n",
      "std too big\n",
      "\n",
      "5\n",
      "clinical, pneumonia, (2019-ncov), infected, patients\n",
      "-2.148265242576599 1.0412233471870422 18\n",
      "0.1264463265593748 0.1069765471027824 0.16562806262947674\n",
      "nan\n",
      "std too big\n",
      "\n",
      "6\n",
      "spike, binding, receptor, therapeutic, sars\n",
      "-3.5661362409591675 7.448850154876709 8\n",
      "0.1336634493357603 0.04214164650858651 0.1401493348496097\n",
      "nan\n",
      "std too big\n",
      "\n",
      "7\n",
      "data, covid-19, risk\n",
      "-0.7805247902870178 -7.493217468261719 7\n",
      "0.023245295416428276 0.08050060940589902 0.08378956900305753\n",
      "11.358477970627503\n",
      "\n",
      "8\n",
      "estimation, risk, analysis, novel_coronavirus_(2019-ncov), early\n",
      "3.521720290184021 -1.4890528917312622 10\n",
      "0.090501582562904 0.10275301121663957 0.13692595721950235\n",
      "18.47196261682243\n",
      "std too big\n"
     ]
    }
   ],
   "source": [
    "##### clDict = pickle.load(open(\"clDict.p\", \"rb\"))\n",
    "\n",
    "label_source_df = pd.DataFrame(index=clDict.keys(), columns=[\"x\", \"y\", \"text\"])\n",
    "\n",
    "for cl_id, cluster in clDict.items():\n",
    "    #print(cluster)\n",
    "    cl_dois = cluster[\"dois\"]\n",
    "    x = np.zeros(len(cl_dois))\n",
    "    y = np.zeros(len(cl_dois))\n",
    "    w = np.zeros(len(cl_dois))\n",
    "    r = np.zeros(len(cl_dois))\n",
    "    g = np.zeros(len(cl_dois))\n",
    "    b = np.zeros(len(cl_dois))\n",
    "    n = 0\n",
    "    \n",
    "    \n",
    "    for i, doi in enumerate(cl_dois):\n",
    "        if doi in df.index:\n",
    "            n += 1\n",
    "            x[i] = df.at[doi, \"xval\"]\n",
    "            y[i] = df.at[doi, \"yval\"]\n",
    "            w[i] = df.at[doi, \"refCountNormalized\"]\n",
    "        else:\n",
    "            w[i] = 0\n",
    "    #if n < 3:\n",
    "    #    continue\n",
    "    \n",
    "    xx = np.median(x)\n",
    "    yy = np.median(y)\n",
    "    \n",
    "    #xx = np.average(x, weights=w)\n",
    "    #yy = np.average(y, weights=w)\n",
    "    \n",
    "    x_std = np.std(x/x_span)\n",
    "    y_std = np.std(y/y_span)\n",
    "    \n",
    "    print()\n",
    "    print(cl_id)\n",
    "    print(\", \".join([s for s, totalFreq, clFreq, v in cluster[\"vec\"] if v>=0.1 and totalFreq>=3][:5]))\n",
    "    print(xx,yy,len(cl_dois))\n",
    "    print(x_std, y_std , np.sqrt(x_std**2 + y_std**2))\n",
    "    print(np.sum(w))\n",
    "    \n",
    "    \n",
    "    if np.sum(w) < 10:\n",
    "        print(\"w too small\")\n",
    "        continue\n",
    "    \n",
    "    std_max = 0.1\n",
    "    \n",
    "    if x_std**2 + y_std**2 > std_max**2:\n",
    "        print(\"std too big\")\n",
    "        #continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_source_df.at[cl_id, \"x\"] = xx\n",
    "    label_source_df.at[cl_id, \"y\"] = yy\n",
    "    #label_source.at[cl_id, \"text\"] = \", \".join([s for s, n in cluster[\"top_authors\"][:3]]) \\\n",
    "    #    + \" | \" + \",\\n\".join([s for s, totalFreq, clFreq, v in cluster[\"vec\"] if v>=0.25 and clFreq>=3][:2])\n",
    "    \n",
    "    #label_source_df.at[cl_id, \"text\"] = \", \".join([s for s, totalFreq, clFreq, v in cluster[\"vec\"] if v>=0.3])\n",
    "    label_source_df.at[cl_id, \"text\"] = \", \".join([s for s, totalFreq, clFreq, v in cluster[\"vec\"] if v>=0.1 and totalFreq>=3][:2])\n",
    "    #label_source.at[cl_id, \"text\"] = \", \".join([s for s, totalFreq, clFreq, v in cluster[\"vec\"] if v>=0.225 and clFreq>=2][:2])\n",
    "    #label_source.at[cl_id, \"text\"] = \", \".join([s for s, totalFreq, clFreq, v in cluster[\"vec\"] if v>=0.01 and clFreq>=1][:1])\n",
    "    #label_source.at[cl_id, \"text\"] = \", \".join([s for s, totalFreq, clFreq, v in cluster[\"vec\"]][:1])\n",
    "    #label_source.at[cl_id, \"text\"] = \", \".join([s for s, totalFreq, clFreq, v in cluster[\"vec\"] if v>=500])\n",
    "    if len(label_source_df.at[cl_id, \"text\"]) == 0:\n",
    "        label_source_df.at[cl_id, \"text\"] = np.nan\n",
    "    \n",
    "    top_authors = cluster[\"top_authors\"]\n",
    "    label_source_df.at[cl_id, \"authors\"] = \", \".join([a for a,n in top_authors][:3])\n",
    "    \n",
    "    #print(xx,yy,len(cl_dois),label_source_df.at[cl_id, \"text\"])\n",
    "    \n",
    "\n",
    "                 \n",
    "    #xy = X_embedded[, dtype=int), :]\n",
    "    #x = df[\"xval\"].loc[cl_dois]\n",
    "    #print()\n",
    "\n",
    "#label_source = \n",
    "label_source_df = label_source_df[pd.notnull(label_source_df[\"x\"])]\n",
    "label_source_df = label_source_df[pd.notnull(label_source_df[\"text\"])]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 17:03:33,906 : INFO : Session output file 'test.html' already exists, will be overwritten.\n"
     ]
    }
   ],
   "source": [
    "from bokeh.plotting import figure, output_file, show, save, output_notebook\n",
    "from bokeh.models import HoverTool, Range1d, LabelSet, LassoSelectTool, CustomJS, TapTool, OpenURL\n",
    "from bokeh.models.sources import ColumnDataSource\n",
    "from bokeh.io import reset_output\n",
    "\n",
    "reset_output()\n",
    "\n",
    "\n",
    "#class hover(HoverTool):\n",
    "#    hover.tooltips = [\n",
    "#    (\"index\", \"$index\"),\n",
    "\n",
    "\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [\n",
    "    (\"Title\", \"@title\"),\n",
    "    (\"Autoren\", \"@author\"),\n",
    "    (\"Verffentlicht\", \"@issued\"),\n",
    "    (\"Journal\", \"@containerTitle\"),\n",
    "    (\"Referenzen\", \"@refCount\"),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "s1 = ColumnDataSource(data=df)\n",
    "\n",
    "\n",
    "callback = CustomJS(code=\"\"\"\n",
    "// the event that triggered the callback is cb_obj:\n",
    "// The event type determines the relevant attributes\n",
    "console.log('Tap event occured at x-position: ' + cb_obj.x)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def lasso_cb(x):\n",
    "    print(x)\n",
    "\n",
    "#lasso = LassoSelectTool(callback=lasso_cb)\n",
    "lasso = LassoSelectTool()\n",
    "\n",
    "\n",
    "p = figure(plot_width=1200, plot_height=1000, active_scroll = \"wheel_zoom\")\n",
    "#p = figure(plot_width=1200, plot_height=1000, output_backend=\"webgl\", active_scroll = \"wheel_zoom\")\n",
    "p.circle(x='xval', y='yval', size=\"scatterSize\", color=\"rgb\", alpha=\"alpha\", source=s1)\n",
    "\n",
    "p.add_tools(hover)\n",
    "p.add_tools(lasso)\n",
    "\n",
    "p.js_on_event('tap', callback)\n",
    "\n",
    "#p.toolbar.active_scroll = \"wheel_zoom\"\n",
    "\n",
    "df[\"xval\"].max()\n",
    "\n",
    "left, right, bottom, top = -120, 120, -90, 90\n",
    "left, right, bottom, top = -80, 80, -60, 60\n",
    "left, right, bottom, top = -100, 100, -90, 90\n",
    "p.x_range=Range1d(left, right)\n",
    "p.y_range=Range1d(bottom, top)\n",
    "\n",
    "try:\n",
    "    label_source = ColumnDataSource(label_source_df)   \n",
    "\n",
    "    p.scatter(x='x', y='y', source=label_source, size=8, color=\"black\", marker=\"diamond\")\n",
    "    \n",
    "    labels = LabelSet(x='x', y='y', x_offset=-20, y_offset=5, text='text', level='glyph', \n",
    "                  source=label_source, render_mode='canvas')\n",
    "    #author_labels = LabelSet(x='x', y='y', x_offset=10, y_offset=-15, text='authors', level='glyph', \n",
    "    #              source=label_source, render_mode='canvas')\n",
    "\n",
    "    p.add_layout(labels)\n",
    "    #p.add_layout(author_labels)\n",
    "except ValueError:\n",
    "    pass\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "output_file(\"test.html\")\n",
    "#output_notebook()\n",
    "#show(p)\n",
    "#save(p)\n",
    "\n",
    "from bokeh.layouts import widgetbox, column, row\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.models.widgets import DataTable, DateFormatter, TableColumn\n",
    "\n",
    "s2 = ColumnDataSource(df)\n",
    "\n",
    "columns = [\n",
    "        TableColumn(field=\"clusterNo\", title=\"Cluster\"),\n",
    "        TableColumn(field=\"title\", title=\"Title\"),\n",
    "        TableColumn(field=\"author\", title=\"Autoren\"),\n",
    "        TableColumn(field=\"issued\", title=\"Verffentlichungsjahr\"),\n",
    "    ]\n",
    "data_table = DataTable(source=s1, columns=columns, width=640, height=800)\n",
    "\n",
    "#p.add_widget(widgetbox(data_table))\n",
    "\n",
    "#show(p)\n",
    "\n",
    "\n",
    "#s1.callback = \n",
    "jscode = CustomJS(args=dict(s2=s2), code=\"\"\"\n",
    "    var inds = cb_obj.get('selected')['1d'].indices;\n",
    "    var d1 = cb_obj.get('data');\n",
    "    var d2 = s2.get('data');\n",
    "    \n",
    "    \n",
    "    d2[\"title] = []\n",
    "    d2[\"author] = []\n",
    "    d2[\"issued] = []\n",
    "    for (i = 0; i < inds.length; i++) {\n",
    "        d2[\"title\"].push(d1[\"title\"][inds[i]])\n",
    "        d2[\"author\"].push(d1[\"author\"][inds[i]])\n",
    "        d2[\"issued\"].push(d1[\"issued\"][inds[i]])\n",
    "    }\n",
    "    s2.trigger('change');\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "p.add_tools(TapTool(callback=OpenURL(url=\"http://doi.org/@doi\")))\n",
    "\n",
    "# use the \"color\" column of the CDS to complete the URL\n",
    "# e.g. if the glyph at index 10 is selected, then @color\n",
    "# will be replaced with source.data['color'][10]\n",
    "#url = \"http://doi.org/@doi\"\n",
    "#taptool = p.select(type=TapTool)\n",
    "##taptool = TapTool()\n",
    "#taptool.callback = OpenURL(url=url)\n",
    "##p.add_tools(taptool)\n",
    "\n",
    "layout = row(p, widgetbox(data_table))\n",
    "#layout = column(p, widgetbox(data_table))\n",
    "\n",
    "show(layout)\n",
    "#save(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-49-6aaf1f276005>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-49-6aaf1f276005>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(s1)\n",
    "#s1.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "label_source\n",
    "#type(label_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "        TableColumn(field=\"title\", title=\"Title\"),\n",
    "        TableColumn(field=\"author\", title=\"Autoren\"),\n",
    "        TableColumn(field=\"issued\", title=\"Jahr\", formatter=DateFormatter()),\n",
    "    ]\n",
    "data_table = DataTable(source=ColumnDataSource(df), columns=columns, width=400, height=280)\n",
    "\n",
    "show(widgetbox(data_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all = pd.DataFrame(index=set(k for k in work_dict.keys() if k in df), columns=[\"doi\", \"clusterID\", \"issued\", \"title\", \"authors\", \"containerTitles\", \"refCount\", \"link\", \"inPlot\"])\n",
    "#for i, doi in enumerate(work_dict.keys()):\n",
    "\n",
    "#df_dois = set(dois_to_plot & work_dict.keys())\n",
    "df_dois = set(reference_vectors.vocab.keys() & work_dict.keys())\n",
    "\n",
    "df_all = pd.DataFrame(index=df_dois, columns=[\"doi\", \"issued\", \"issuedYear\", \"title\", \"authors\", \"containerTitles\", \"refCount\", \"refCountNormalized\", \"link\", \"clusterID\", \"clusterKWs\", \"inPlot\"])\n",
    "for i, doi in enumerate(df_dois):\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(i, len(df_dois))\n",
    "    \n",
    "    pub = work_dict[doi]\n",
    "    #print(pub)\n",
    "    df_all.at[doi, \"doi\"] = doi\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        df_all.at[doi, \"title\"] = pub[\"title\"][0]\n",
    "    except IndexError:\n",
    "        pass\n",
    "        #df_all.at[doi, \"title\"] = None\n",
    "    except KeyError:\n",
    "        pass\n",
    "    df_all.at[doi, \"issued\"] = \"-\".join([str(x) for x in pub[\"issued\"][\"date-parts\"][0]])\n",
    "    df_all.at[doi, \"issuedYear\"] = pub[\"issued\"][\"date-parts\"][0][0]\n",
    "    df_all.at[doi, \"refCount\"] = ref_counts[doi]\n",
    "    try:\n",
    "        pub_authors = [a[\"family\"] for a in pub[\"author\"]]\n",
    "        df_all.at[doi, \"authors\"] = \", \".join(pub_authors)\n",
    "    except KeyError:\n",
    "        df_all.at[doi, \"authors\"] = \"unavailable\"\n",
    "    try:\n",
    "        df_all.at[doi, \"containerTitles\"] = pub[\"container-title\"]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        df_all.at[doi, \"link\"] = \"=Hyperlink(\\\"https://doi.org/{}\\\")\".format(doi)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    if doi in dois_to_plot:\n",
    "        df_all.at[doi, \"inPlot\"] = True\n",
    "        clid = df.at[doi, \"clusterNo\"]\n",
    "        df_all.at[doi, \"clusterID\"] = clid\n",
    "        if clid in clDict:\n",
    "            df_all.at[doi, \"clusterKWs\"] = \", \".join([s for s, totalFreq, clFreq, v in clDict[clid][\"vec\"] if v>=0.1 and totalFreq>=3])\n",
    "    else:\n",
    "        df_all.at[doi, \"inPlot\"] = False\n",
    "        \n",
    "    try:\n",
    "        year = pub[\"issued\"][\"date-parts\"][0][0]\n",
    "        x = ref_counts[doi]\n",
    "        if year in lambda_mle_ref_count:\n",
    "            l = lambda_mle_ref_count[year]\n",
    "        else:\n",
    "            l = lambda_mle_ref_count_default\n",
    "        #df_all.at[doi, \"expRefCountPercentile\"] = 1. - np.exp(-l*x)\n",
    "        df_all.at[doi, \"refCountNormalized\"] = l*x\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.sort_values(by=\"refCount\", ascending=False)\n",
    "df_all.sample(10)\n",
    "df_all.to_excel(\"df_all.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_inPlot = df_all[df_all[\"inPlot\"]==True]\n",
    "len(df_all_inPlot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_inPlot.to_excel(\"df_all_inPlot.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dois_to_plot.index(\"10.1021/op200097d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idee:\n",
    "\n",
    "fr Bestimmung der Wahrscheinlichkeit fr Referenz eines Papers bei einer vorhandenen Menge an bekannten Referenzen\n",
    "\n",
    "Kreuzvalidierung mglicher Metriken mit Subset der Referenz-Relationen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
